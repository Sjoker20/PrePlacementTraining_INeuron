{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eaca388",
   "metadata": {},
   "source": [
    "***General Linear Model:***\n",
    "\n",
    "**1. What is the purpose of the General Linear Model (GLM)?**\n",
    "<aside>Ans: - General Linear Model (GLM) is a statistical framework used to model the relationship \n",
    "    between a dependent variable and one or more independent variables. It provides a flexible approach \n",
    "    to analyze and understand the relationships between variables, making it widely used in various fields \n",
    "    such as regression analysis, analysis of variance (ANOVA), and analysis of covariance (ANCOVA)</aside>\n",
    "\n",
    "**2. What are the key assumptions of the General Linear Model?**\n",
    "    \n",
    "<aside>Ans:- Linearity,Independance,Homoscedasticity,Normality,No Multicollinearity,No Endogeneity,Correct Specification\n",
    "</aside>\n",
    "    \n",
    "**3. How do you interpret the coefficients in a GLM?**\n",
    "    \n",
    "<aside>Ans:- The interpretation of coefficients can be achieved by following measure:\n",
    "    Coefficient Sign, magnitude, statistical significance,Adjusted vs. Unadjusted Coefficients\n",
    "</aside>\n",
    "    \n",
    "**4. What is the difference between a univariate and multivariate GLM?**\n",
    "\n",
    "<aside>Ans:- In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix\n",
    "</aside>\n",
    "    \n",
    "**5. Explain the concept of interaction effects in a GLM.**\n",
    "\n",
    "<aside>Ans:-The interaction effect between two variables is equal to the product term coefficient between these variables.\n",
    "    </aside>\n",
    "    \n",
    "**6. How do you handle categorical predictors in a GLM?**\n",
    "    \n",
    "<aside>Ans:- Categorical predictors can be handles using following methods:\n",
    "    Dummy coding(Binary coding), Effect Coding(Deviation coding) or One-Hot encoding\n",
    "    </aside>\n",
    "\n",
    "**7. What is the purpose of the design matrix in a GLM?**\n",
    "    \n",
    "<aside>Ans:-The design matrix, also known as the model matrix or feature matrix, is a crucial component of the General Linear Model (GLM). It is a structured representation of the independent variables in the GLM, organized in a matrix format. The design matrix serves the purpose of encoding the relationships between the independent variables and the dependent variable, allowing the GLM to estimate the coefficients and make predictions\n",
    "    </aside>\n",
    "    \n",
    "**8. How do you test the significance of predictors in a GLM?**\n",
    "    \n",
    "<aside>Ans:-The design matrix allows the GLM to estimate the coefficients for each independent variable. By incorporating the design matrix into the GLM's estimation procedure, the model determines the relationship between the independent variables and the dependent variable, estimating the magnitude and significance of the effects of each predictor.\n",
    "    </aside>\n",
    "    \n",
    "**9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?**\n",
    "    \n",
    "<aside>Ans:-Type I sum of squares are “sequential.” In essence the factors are tested in the order they are listed in the model.\n",
    "    \n",
    "Type III sum of squares are “partial.” In essence, every term in the model is tested in light of every other term in the model. That means that main effects are tested in light of interaction terms as well as in light of other main effects.\n",
    "    \n",
    "Type II sum of squares are similar to Type III, except that they preserve the principle of marginality.  This means that main factors are tested in light of one another, but not in light of the interaction term.\n",
    "    </aside> \n",
    "    \n",
    "**10. Explain the concept of deviance in a GLM.**\n",
    "\n",
    "<aside>Ans:-The deviance is used to compare two models – in particular in the case of generalized linear models (GLM) where it has a similar role to residual sum of squares (RSS) from ANOVA in linear models.\n",
    "    </aside>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a91c46",
   "metadata": {},
   "source": [
    "***Regression:***\n",
    "\n",
    "**11. What is regression analysis and what is its purpose?**\n",
    "\n",
    "<aside>Ans:-Regression analysis is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It aims to understand how changes in the independent variables are associated with changes in the dependent variable. Regression analysis helps in predicting and estimating the values of the dependent variable based on the values of the independent variables.\n",
    "</aside>\n",
    "\n",
    "**12. What is the difference between simple linear regression and multiple linear regression?**\n",
    "\n",
    "<aside>Ans:-Simple linear regression involves a single independent variable (X) and a continuous dependent variable (Y). It models the relationship between X and Y as a straight line while Multiple linear regression involves two or more independent variables (X1, X2, X3, etc.) and a continuous dependent variable (Y)\n",
    "</aside>\n",
    "\n",
    "**13. How do you interpret the R-squared value in regression?**\n",
    "\n",
    "<aside>Ans:-R-squared is a widely used measure to assess the goodness of fit in regression. It represents the proportion of the variance in the dependent variable that can be explained by the independent variables in the model. R-squared ranges from 0 to 1, with a higher value indicating a better fit.\n",
    "</aside>\n",
    "\n",
    "**14. What is the difference between correlation and regression?**\n",
    "\n",
    "<aside>Ans:-The key difference between correlation and regression is that correlation measures the degree of a relationship between two independent variables (x and y). In contrast, regression is how one variable affects another.\n",
    "</aside>\n",
    "g\n",
    "\n",
    "**15. What is the difference between the coefficients and the intercept in regression?**\n",
    "\n",
    "<aside>Ans:-Coefficients provide information about the magnitude and direction of the effect that each independent variable has on the dependent variable, assuming all other variables in the model are held constant while the intercept (sometimes called the “constant”) in a regression model represents the mean value of the response variable when all of the predictor variables in the model are equal to zero.\n",
    "</aside>\n",
    "\n",
    "**16. How do you handle outliers in regression analysis?**\n",
    "\n",
    "<aside>Ans:-Outliers can be handled with the following way:\n",
    "    1. Remove the values\n",
    "    2. Replace the value with median or maximum element present in the dataset\n",
    "    3. Assign a dummy variable\n",
    "</aside>\n",
    "\n",
    "**17. What is the difference between ridge regression and ordinary least squares regression?**\n",
    "\n",
    "<aside>Ans:- OLS aims to minimize the sum of squared residuals and finds the best-fit coefficients for the predictors while Ridge Regression adds a penalty term known as the regularization parameter, to the sum of squared residuals to control the magnitude of the coefficients. In summary, when there is a difference in variance between predictor variables, OLS tends to give higher variance for coefficients corresponding to predictors with higher variance, while Ridge Regression reduces the variance differences between coefficients by shrinking them towards zero.\n",
    "</aside>\n",
    "\n",
    "**18. What is heteroscedasticity in regression and how does it affect the model?**\n",
    "\n",
    "<aside>Ans:-Heteroscedasticity, where the variance of the errors varies with the levels of the predictors, violates this assumption and can impact the validity of statistical tests and confidence intervals.\n",
    "</aside>\n",
    "\n",
    "**19. How do you handle multicollinearity in regression analysis?**\n",
    "\n",
    "<aside>Ans:-Multicollinealrility in regression model can be addressed by following methods:-\n",
    "1. Variable Selection: Remove one or more correlated variables from the regression model to eliminate multicollinearity. Prioritize variables that are theoretically more relevant or have stronger relationships with the dependent variable.\n",
    "2. Data Collection: Collect additional data to reduce the correlation between variables. Increasing sample size can help alleviate multicollinearity by providing a more diverse range of observations.\n",
    "3. Ridge Regression: Use regularization techniques like ridge regression to mitigate multicollinearity. Ridge regression introduces a penalty term that shrinks the coefficient estimates, reducing their sensitivity to multicollinearity.\n",
    "4. Principal Component Analysis (PCA): Transform the correlated variables into a set of uncorrelated principal components through techniques like PCA. The principal components can then be used as independent variables in the regression model.\n",
    "    Addressing multicollinearity is essential to ensure the accuracy and reliability of regression analysis. By identifying and managing multicollinearity, we can better understand the individual effects of independent variables and improve the interpretability of the regression model.\n",
    "\n",
    "</aside>\n",
    "\n",
    "**20. What is polynomial regression and when is it used?**\n",
    "\n",
    "<aside>Ans:-A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. \n",
    "    It is used when linear regression models may not adequately capture the complexity of the relationship\n",
    "</aside>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ea807",
   "metadata": {},
   "source": [
    "***Loss function:***\n",
    "\n",
    "**21. What is a loss function and what is its purpose in machine learning?**\n",
    "\n",
    "<aside>Ans:-A loss function, also known as a cost function or objective function, is a measure used to quantify the discrepancy or error between the predicted values and the true values in a machine learning or optimization problem. The choice of a suitable loss function depends on the specific task and the nature of the problem.\n",
    "    The purpose of a loss function in machine learning algorithms is to quantify the discrepancy or error between the predicted outputs and the true values in order to guide the learning process. Loss functions play a crucial role in training models by providing a measure of how well the model is performing and allowing optimization algorithms to adjust the model's parameters to minimize the error. \n",
    "</aside>\n",
    "\n",
    "**22. What is the difference between a convex and non-convex loss function?**\n",
    "\n",
    "<aside>Ans:- A function is said to be convex if any line segment connecting two points on the graph of the function lies entirely above the graph. In other words, a function is convex if it always bends upward.\n",
    "    In contrast to convex loss functions, non-convex loss functions have multiple local minima and may be challenging to optimize. Non-convexity can pose challenges in finding the global minimum as optimization algorithms may get stuck in suboptimal solutions\n",
    "</aside>\n",
    "\n",
    "**23. What is mean squared error (MSE) and how is it calculated?**\n",
    "\n",
    "<aside>Ans:-The Mean Squared Error is a commonly used loss function for regression problems. It calculates the average of the squared differences between the predicted and true values. The goal is to minimize the MSE, which penalizes larger errors more severely.\n",
    "    Mathematically, the squared loss is defined as:\n",
    "Loss(y, ŷ) = (1/n) * ∑(y - ŷ)^2\n",
    "\n",
    "</aside>\n",
    "\n",
    "**24. What is mean absolute error (MAE) and how is it calculated?**\n",
    "\n",
    "<aside>Ans:-Absolute loss, also known as Mean Absolute Error (MAE), measures the average of the absolute differences between the predicted and true values. It treats all errors equally, regardless of their magnitude, making it less sensitive to outliers compared to squared loss.\n",
    "    Mathematically, the absolute loss is defined as:\n",
    "Loss(y, ŷ) = (1/n) * ∑|y - ŷ|\n",
    "\n",
    "</aside>\n",
    "\n",
    "**25. What is log loss (cross-entropy loss) and how is it calculated?**\n",
    "\n",
    "<aside>Ans:- Binary Cross-Entropy loss is commonly used for binary classification problems, where the goal is to classify instances into two classes. It quantifies the difference between the predicted probabilities and the true binary labels.\n",
    "</aside>\n",
    "\n",
    "**26. How do you choose the appropriate loss function for a given problem?**\n",
    "\n",
    "<aside>Ans:-An appropriate loss function for a given problem involves considering the nature of the problem, the type of learning task (regression, classification, etc.), and the specific goals or requirements of the problem. For example, MSE or MAE can be chosen for regression type problems while for classification problems Log-loss can be chosen for the analysis of the model.\n",
    "</aside>\n",
    "\n",
    "**27. Explain the concept of regularization in the context of loss functions.**\n",
    "\n",
    "<aside>Ans:- Regularization is a technique used in machine learning to prevent overfitting and improve the generalization ability of a model. It introduces additional constraints or penalties to the loss function, encouraging the model to learn simpler patterns and avoid overly complex or noisy representations. Regularization helps strike a balance between fitting the training data well and avoiding overfitting, thereby improving the model's performance on unseen data. \n",
    "</aside>\n",
    "\n",
    "**28. What is Huber loss and how does it handle outliers?**\n",
    "\n",
    "<aside>Ans:-Huber loss, named after its creator Peter J. Huber, is a robust loss function that combines the best properties of L1 and L2 losses. It is less sensitive to outliers in data than the mean squared error loss, making it ideal for regression problems where data can have large outliers.\n",
    "</aside>\n",
    "\n",
    "**29. What is quantile loss and when is it used?**\n",
    "\n",
    "<aside>Ans:-The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
    "    It is useful in understanding outcomes that are non-normally distributed and that have nonlinear relationships with predictor variables.\n",
    "</aside>\n",
    "\n",
    "**30. What is the difference between squared loss and absolute loss?**\n",
    "\n",
    "<aside>Ans:- The differences between squared loss and absolute loss are mentioned below:\n",
    "- Sensitivity to Errors: Squared loss penalizes larger errors more severely due to the squaring operation, while absolute loss treats all errors equally, regardless of their magnitude.\n",
    "- Sensitivity to Outliers: Squared loss is more sensitive to outliers because the squared differences amplify the impact of extreme values. Absolute loss is less sensitive to outliers as it only considers the absolute differences.\n",
    "- Differentiability: Squared loss is differentiable, making it suitable for gradient-based optimization algorithms. Absolute loss is not differentiable at zero, which may require specialized optimization techniques.\n",
    "- Robustness: Absolute loss is more robust to outliers and can provide more robust estimates in the presence of extreme values compared to squared loss.\n",
    "\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ebdca",
   "metadata": {},
   "source": [
    "***Optimizer (GD):***\n",
    "\n",
    "**31. What is an optimizer and what is its purpose in machine learning?**\n",
    "\n",
    "<aside>Ans:-An optimizer is an algorithm or method used to adjust the parameters of a model in order to minimize the loss function or maximize the objective function.\n",
    "    Optimizers play a crucial role in training machine learning models by iteratively updating the model's parameters to improve its performance. They determine the direction and magnitude of the parameter updates based on the gradients of the loss or objective function\n",
    "</aside>\n",
    "\n",
    "**32. What is Gradient Descent (GD) and how does it work?**\n",
    "\n",
    "<aside>Ans:-Gradient Descent (GD) is an optimization algorithm used to minimize the loss function and update the parameters of a machine learning model iteratively. \n",
    "    It works by iteratively adjusting the model's parameters in the direction opposite to the gradient of the loss function. \n",
    "</aside>\n",
    "\n",
    "\n",
    "**33. What are the different variations of Gradient Descent?**\n",
    "\n",
    "<aside>Ans:- Below are the gradient descent variants:\n",
    "-Stochastic Gradient Descent (SGD): This variant randomly samples a subset of the training data (a batch) in each iteration, making the updates more frequent but with higher variance.\n",
    "- Mini-Batch Gradient Descent: This variant combines the benefits of SGD and batch gradient descent by using a mini-batch of data for each parameter update.\n",
    "-Batch Graident Descent: Batch Gradient Descent computes the gradients using the entire training dataset in each iteration. It calculates the average gradient over all training examples and updates the parameters accordingly.\n",
    "</aside>\n",
    "\n",
    "**34. What is the learning rate in GD and how do you choose an appropriate value?**\n",
    "\n",
    "<aside>Ans:- Learning rate in gradient descent determines how fast or slow we will move towards the optimal weights. \n",
    "    This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum\n",
    "</aside>\n",
    "\n",
    "**35. How does GD handle local optima in optimization problems?**\n",
    "\n",
    "<aside>Ans:-Momentum is a technique in gradient descent that helps overcome local minima and accelerates convergence. It introduces a \"momentum\" term that accumulates the gradients over time. In addition to the learning rate, you need to tune the momentum hyperparameter. Higher values of momentum (e.g., 0.9) can smooth out the update trajectory and help navigate flat regions, while lower values (e.g., 0.5) allow for more stochasticity\n",
    "</aside>\n",
    "\n",
    "**36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?**\n",
    "\n",
    "<aside>Ans:- Stochastic Gradient Descent is a drastic simplification of GD which overcomes some of its difficulties. Each iteration of SGD computes the gradient on the basis of one randomly chosen partition of the dataset which was shuffled, instead of using the whole part of the observations. This modification of GD can reduce the computational time significantly.\n",
    "    It computes graident based on partition of dataset while gradient descent computes the gradient on a whole dataset.\n",
    "</aside>\n",
    "\n",
    "**37. Explain the concept of batch size in GD and its impact on training.**\n",
    "\n",
    "<aside>Ans:- Batch size defines the size of dataset used for computing the gradient. The different GD varaiations may perform based on the batch size of the dataset, like Batch GD could be complex and requires high computation time rather than SGD or mini-batch gd.\n",
    "</aside>\n",
    "\n",
    "**38. What is the role of momentum in optimization algorithms?**\n",
    "\n",
    "<aside>Ans:-Momentum is a technique that helps overcome local minima and accelerates convergence. It introduces a \"momentum\" term that accumulates the gradients over time. In addition to the learning rate, you need to tune the momentum hyperparameter\n",
    "</aside>\n",
    "\n",
    "**39. What is the difference between batch GD, mini-batch GD, and SGD?**\n",
    "\n",
    "<aside>Ans:-Variations like SGD and mini-batch gradient descent are often preferred for large-scale and deep learning tasks due to their efficiency, while BGD is suitable for smaller datasets or problems where convergence to the global minimum is desired\n",
    "</aside>\n",
    "\n",
    "**40. How does the learning rate affect the convergence of GD?**\n",
    "\n",
    "<aside>Ans:-The step size or learning rate in optimization algorithms also plays a role in convergence. A suitable step size ensures that the algorithm makes progress towards the minimum without overshooting or oscillating around it. Convergence requires finding the right balance between larger steps for faster progress and smaller steps for fine-tuning near the minimum\n",
    "</aside>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8de210",
   "metadata": {},
   "source": [
    "***Regularization:***\n",
    "\n",
    "**41. What is regularization and why is it used in machine learning?**\n",
    "\n",
    "<aside>Ans:-Regularization is a technique used in machine learning to prevent overfitting and improve the generalization ability of a model. It introduces additional constraints or penalties to the loss function, encouraging the model to learn simpler patterns and avoid overly complex or noisy representations.\n",
    "    Regularization helps strike a balance between fitting the training data well and avoiding overfitting, thereby improving the model's performance on unseen data.\n",
    "</aside>\n",
    "\n",
    "**42. What is the difference between L1 and L2 regularization?**\n",
    "\n",
    "<aside>Ans:-L1 regularization (Lasso regression) can be used to penalize the absolute values of the regression coefficients. It encourages the model to select only the most important features while shrinking the coefficients of less relevant features to zero. This helps in feature selection and avoids overfitting by reducing the model's complexity.\n",
    "    L2 regularization (Ridge regression) can be used to penalize the squared values of the regression coefficients. It leads to smaller coefficients for less influential features and improves the model's generalization ability by reducing the impact of noisy or irrelevant features.\n",
    "</aside>\n",
    "\n",
    "**43. Explain the concept of ridge regression and its role in regularization.**\n",
    "\n",
    "<aside>Ans:-L2 regularization adds a penalty term to the loss function proportional to the square of the model's coefficients. It encourages the model to reduce the magnitude of all coefficients uniformly, effectively shrinking them towards zero without necessarily setting them exactly to zero. L2 regularization can be represented as:\n",
    "Loss function + λ * ||coefficients||₂²\n",
    "\n",
    "</aside>\n",
    "\n",
    "**44. What is the elastic net regularization and how does it combine L1 and L2 penalties?**\n",
    "\n",
    "<aside>Ans:-Elastic Net regularization combines both L1 and L2 regularization techniques. It adds a linear combination of the L1 and L2 penalty terms to the loss function, controlled by two hyperparameters: α and λ. Elastic Net can overcome some limitations of L1 and L2 regularization and provides a balance between feature selection and coefficient shrinkage.\n",
    "</aside>\n",
    "\n",
    "**45. How does regularization help prevent overfitting in machine learning models?**\n",
    "\n",
    "<aside>Ans:-Regularization is particularly important when dealing with limited or noisy data, complex models with high-dimensional feature spaces, and cases where the number of features exceeds the number of observations. By adding regularization, machine learning models can effectively balance complexity and simplicity, leading to improved generalization performance, more stable and interpretable models, and reduced overfitting.\n",
    "</aside>\n",
    "\n",
    "**46. What is early stopping and how does it relate to regularization?**\n",
    "\n",
    "<aside>Ans:-Early stopping can be thought of as implicit regularization, contrary to regularization via weight decay. This method is also efficient since it requires less amount of training data, which is not always available.\n",
    "</aside>\n",
    "\n",
    "**47. Explain the concept of dropout regularization in neural networks.**\n",
    "\n",
    "<aside>Ans:-Dropout regularization is a technique primarily used in neural networks. It randomly drops out (sets to zero) a fraction of neurons or connections during each training iteration. Dropout prevents the network from relying too heavily on a specific subset of neurons and encourages the learning of more robust and generalizable features\n",
    "</aside>\n",
    "\n",
    "**48. How do you choose the regularization parameter in a model?**\n",
    "\n",
    "<aside>Ans:-Every regularization technique has its advantages and implications, and the choice depends on the specific problem, the nature of the data, and the model architecture. Regularization is an essential tool to prevent overfitting, improve generalization, and balance model complexity in machine learning.\n",
    "</aside>\n",
    "\n",
    "**49. What is the difference between feature selection and regularization?**\n",
    "\n",
    "<aside>Ans:-Feature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. Regularization, where we are constraining the solution space while doing optimization.\n",
    "</aside>\n",
    "\n",
    "**50. What is the trade-off between bias and variance in regularized models?**\n",
    "\n",
    "<aside>Ans:-If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522497e6",
   "metadata": {},
   "source": [
    "***SVM:***\n",
    "\n",
    "**51. What is Support Vector Machines (SVM) and how does it work?**\n",
    "\n",
    "<aside>Ans:-Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. It is particularly effective for solving binary classification problems but can be extended to handle multi-class classification as well. SVM aims to find an optimal hyperplane that maximally separates the classes or minimizes the regression error. \n",
    "</aside>\n",
    "\n",
    "**52. How does the kernel trick work in SVM?**\n",
    "\n",
    "<aside>Ans:-The kernel trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space. It allows SVM to find a linear decision boundary in the transformed feature space without explicitly computing the coordinates of the transformed data points. This enables SVM to solve complex classification problems that cannot be linearly separated in the original input space. \n",
    "</aside>\n",
    "\n",
    "**53. What are support vectors in SVM and why are they important?**\n",
    "\n",
    "<aside>Ans:-Support vectors are the data points that are closest to the decision boundary or lie on the wrong side of the margin. These points play a crucial role in defining the hyperplane. SVM algorithm focuses only on these support vectors, making it memory efficient and computationally faster than other algorithms\n",
    "</aside>\n",
    "\n",
    "**54. Explain the concept of the margin in SVM and its impact on model performance.**\n",
    "\n",
    "<aside>Ans:-The margin in Support Vector Machines (SVM) is a critical concept that plays a crucial role in determining the optimal decision boundary between classes. The purpose of the margin is to maximize the separation between the support vectors of different classes and the decision boundary\n",
    "</aside>\n",
    "\n",
    "**55. How do you handle unbalanced datasets in SVM?**\n",
    "\n",
    "<aside>Ans:- Unbalanced dataset can be handled in SVM using following techniques:\n",
    "    Class weighting, Oversampling,Undersampling,Combination of sampling techniques,Adjusting decision threshold\n",
    "</aside>\n",
    "\n",
    "**56. What is the difference between linear SVM and non-linear SVM?**\n",
    "\n",
    "<aside>Ans:- - Linear SVM: In a linear SVM, the hyperplane is a straight line. The algorithm finds the optimal hyperplane by maximizing the margin between the support vectors. It aims to find a line that best separates the classes and allows for the largest margin.\n",
    "- Non-linear SVM: In cases where the data points are not linearly separable, SVM can use a kernel trick to transform the input features into a higher-dimensional space, where they become linearly separable. Common kernel functions include polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel\n",
    "\n",
    "</aside>\n",
    "\n",
    "**57. What is the role of C-parameter in SVM and how does it affect the decision boundary?**\n",
    "\n",
    "<aside>Ans:-SVM allows for soft margin classification by introducing a regularization parameter (C). C controls the trade-off between maximizing the margin and minimizing the misclassification of training examples. A higher value of C allows fewer misclassifications (hard margin), while a lower value of C allows more misclassifications (soft margin).\n",
    "</aside>\n",
    "\n",
    "**58. Explain the concept of slack variables in SVM.**\n",
    "\n",
    "<aside>Ans:-The soft margin SVM relaxes the constraint of perfect separation and allows for a certain degree of misclassification to find a more practical decision boundary. It introduces a non-negative regularization parameter C that controls the trade-off between maximizing the margin and minimizing the misclassification errors.\n",
    "</aside>\n",
    "\n",
    "**59. What is the difference between hard margin and soft margin in SVM?**\n",
    "\n",
    "<aside>Ans:-When the data is linearly separable, and we don't want to have any misclassifications, we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier\n",
    "</aside>\n",
    "\n",
    "**60. How do you interpret the coefficients in an SVM model?**\n",
    "\n",
    "<aside>Ans:-Suppose the svm would find only one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation.\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507316e",
   "metadata": {},
   "source": [
    "***Decision Trees:***\n",
    "\n",
    "**61. What is a decision tree and how does it work?**\n",
    "\n",
    "<aside>Ans:-A decision tree is a supervised machine learning algorithm that is used for both classification and regression tasks. It represents a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a prediction. Decision trees are intuitive, interpretable, and widely used due to their simplicity and effectiveness\n",
    "</aside>\n",
    "\n",
    "**62. How do you make splits in a decision tree?**\n",
    "\n",
    "<aside>Ans:-A decision tree makes splits or determines the branching points based on the attribute that best separates the data and maximizes the information gain or reduces the impurity. The process of determining splits involves selecting the most informative attribute at each node.\n",
    "</aside>\n",
    "\n",
    "**63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?**\n",
    "\n",
    "<aside>Ans:-Impurity measures, such as the Gini index and entropy, are used in decision trees to evaluate the homogeneity or impurity of the data at each node. They help determine the attribute that provides the most useful information for splitting the data.\n",
    "</aside>\n",
    "\n",
    "**64. Explain the concept of information gain in decision trees.**\n",
    "\n",
    "<aside>Ans:-Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node\n",
    "</aside>\n",
    "\n",
    "**65. How do you handle missing values in decision trees?**\n",
    "\n",
    "<aside>Ans:- Missing values can be handled in the following ways:\n",
    "    1. Ignore missing values\n",
    "    2. Imputation\n",
    "    3. Predective imputation\n",
    "    4.Splitting Based on Missingness\n",
    "</aside>\n",
    "\n",
    "**66. What is pruning in decision trees and why is it important?**\n",
    "\n",
    "<aside>Ans:-Pruning is a technique used in decision trees to reduce overfitting and improve the model's generalization performance. It involves the removal or simplification of specific branches or nodes in the tree that may be overly complex or not contributing significantly to the overall predictive power. \n",
    "    Pruning helps prevent the decision tree from becoming too specific to the training data, allowing it to better generalize to unseen data.\n",
    "</aside>\n",
    "\n",
    "**67. What is the difference between a classification tree and a regression tree?**\n",
    "\n",
    "<aside>Ans:-Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous\n",
    "</aside>\n",
    "\n",
    "**68. How do you interpret the decision boundaries in a decision tree?**\n",
    "\n",
    "<aside>Ans:-The decision boundary is represented by a tree-like structure where each node represents a decision based on a feature value and each leaf represents a class label.\n",
    "</aside>\n",
    "\n",
    "**69. What is the role of feature importance in decision trees?**\n",
    "\n",
    "<aside>Ans:-Feature importance refers to technique that assigns a score to features based on how significant they are at predicting a target variable. The scores are calculated on the weighted Gini indices. Easy way to obtain the scores is by using the feature_importances_ attribute from the trained tree model\n",
    "</aside>\n",
    "\n",
    "**70. What are ensemble techniques and how are they related to decision trees?**\n",
    "\n",
    "<aside>Ans:-Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a55d49",
   "metadata": {},
   "source": [
    "***Ensemble Techniques:***\n",
    "\n",
    "**71. What are ensemble techniques in machine learning?**\n",
    "\n",
    "<aside>Ans:-Ensemble techniques in machine learning involve combining multiple individual models to create a stronger, more accurate predictive model. Ensemble methods leverage the concept of \"wisdom of the crowd,\" where the collective decision-making of multiple models can outperform any single model.\n",
    "</aside>\n",
    "\n",
    "**72. What is bagging and how is it used in ensemble learning?**\n",
    "\n",
    "<aside>Ans:-Bagging involves training multiple instances of the same base model on different subsets of the training data. Each model learns independently, and their predictions are combined through averaging or voting to make the final prediction\n",
    "</aside>\n",
    "\n",
    "**73. Explain the concept of bootstrapping in bagging.**\n",
    "\n",
    "<aside>Ans:Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method.\n",
    "</aside>\n",
    "\n",
    "**74. What is boosting and how does it work?**\n",
    "\n",
    "<aside>Ans:-Boosting is an ensemble technique in machine learning that sequentially builds an ensemble by training weak models that learn from the mistakes of previous models. The subsequent models give more weight to misclassified instances, leading to improved performance. Boosting focuses on iteratively improving the overall model by combining the predictions of multiple weak learners.\n",
    "</aside>\n",
    "\n",
    "**75. What is the difference between AdaBoost and Gradient Boosting?**\n",
    "\n",
    "<aside>Ans:- AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost\n",
    "</aside>\n",
    "\n",
    "**76. What is the purpose of random forests in ensemble learning?**\n",
    "\n",
    "<aside>Ans:-Random Forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model. The purpose of using Random Forests in ensemble learning is to reduce overfitting, handle high-dimensional data, and improve the stability and predictive performance of the model.\n",
    "</aside>\n",
    "\n",
    "**77. How do random forests handle feature importance?**\n",
    "\n",
    "<aside>Ans:-In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "</aside>\n",
    "\n",
    "**78. What is stacking in ensemble learning and how does it work?**\n",
    "\n",
    "<aside>Ans:-Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance.\n",
    "</aside>\n",
    "\n",
    "**79. What are the advantages and disadvantages of ensemble techniques?**\n",
    "\n",
    "<aside>Ans:-Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data.\n",
    "</aside>\n",
    "\n",
    "**80. How do you choose the optimal number of models in an ensemble?**\n",
    "\n",
    "<aside>Ans:-\n",
    "</aside>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a8cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
